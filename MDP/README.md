## MDP and Bellman Equations

#### Notes:
* Although we define ordering over policies it is important to note that it is only a _partial ordering_, and it's quite easy to come up with an example that breakes an idea of complete ordering. (TODO: think of MDP where all pairs of policies are not orderable, and thereby no optimal policy exist)
* Optimal policy acheives maximum of the _expected_ return, which by defenition contains discounting. This means that in some sense we are allways trying to find best possible suboptimal solution to MDP, implicitly agreeing to this because of our unsertainty in the model.
